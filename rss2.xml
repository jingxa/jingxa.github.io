<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Jingxa&#39;s Blog</title>
    <link>https://jingxa.github.io/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>单影无人相依偎！</description>
    <pubDate>Sat, 08 Dec 2018 08:23:40 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>CS131_Homework_8</title>
      <link>https://jingxa.github.io/2018/12/08/CS131-Homework-8/</link>
      <guid>https://jingxa.github.io/2018/12/08/CS131-Homework-8/</guid>
      <pubDate>Sat, 08 Dec 2018 08:23:40 GMT</pubDate>
      <description>
      
        
        
          &lt;hr&gt;

        
      
      </description>
      
      <content:encoded><![CDATA[<hr>]]></content:encoded>
      
      <comments>https://jingxa.github.io/2018/12/08/CS131-Homework-8/#disqus_thread</comments>
    </item>
    
    <item>
      <title>CS131_Homework_7</title>
      <link>https://jingxa.github.io/2018/12/08/CS131-Homework-7/</link>
      <guid>https://jingxa.github.io/2018/12/08/CS131-Homework-7/</guid>
      <pubDate>Sat, 08 Dec 2018 08:23:23 GMT</pubDate>
      <description>
      
        
        
          &lt;hr&gt;

        
      
      </description>
      
      <content:encoded><![CDATA[<hr>]]></content:encoded>
      
      <comments>https://jingxa.github.io/2018/12/08/CS131-Homework-7/#disqus_thread</comments>
    </item>
    
    <item>
      <title>CS131_Homework_6</title>
      <link>https://jingxa.github.io/2018/12/08/CS131-Homework-6/</link>
      <guid>https://jingxa.github.io/2018/12/08/CS131-Homework-6/</guid>
      <pubDate>Sat, 08 Dec 2018 08:14:56 GMT</pubDate>
      <description>
      
        
        
          &lt;blockquote&gt;
&lt;p&gt;cs131 hw6&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;主要内容&quot;&gt;&lt;a href=&quot;#主要内容&quot; class=&quot;headerlink&quot; title=&quot;主要内容&quot;&gt;&lt;/a&gt;主要内容&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;image c
        
      
      </description>
      
      <content:encoded><![CDATA[<blockquote><p>cs131 hw6</p></blockquote><hr><h1 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h1><ul><li>image compression using SVD</li><li>kNN methods for image recognition.</li><li>PCA and LDA to improve kNN</li></ul><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="https://github.com/wwdguu/CS131_homework/tree/master/hw6_release" target="_blank" rel="noopener">wwdguu/CS131_homework</a></li><li><a href="https://github.com/Hugstar/Solutions-Stanford-cs131-Computer-Vision-Foundations-and-Application/tree/master/hw6_release" target="_blank" rel="noopener">Solutions-Stanford-cs131-Computer-Vision-Foundations-and-Application/hw6_release/</a></li></ul><h1 id="1-Image-Compression"><a href="#1-Image-Compression" class="headerlink" title="1. Image  Compression"></a>1. Image  Compression</h1><h2 id="1-1-使用SVD压缩图片"><a href="#1-1-使用SVD压缩图片" class="headerlink" title="1.1 使用SVD压缩图片"></a>1.1 使用SVD压缩图片</h2><p>函数： <code>def compress_image(image, num_values)</code><br>参数：</p><ul><li>images: (H,w)</li><li>num_values: 保留的奇异值<br>返回值：</li><li>compressed_image: (H,w) 压缩后的图片</li><li>compressed_size: 压缩图片的size</li></ul><p>实现：</p><ol><li>Get SVD of the image</li><li>Only keep the top <code>num_values</code> singular values, and compute <code>compressed_image</code></li><li>Compute the compressed size</li></ol><pre><code class="python">def compress_image(image, num_values):    compressed_image = None    compressed_size = 0    # YOUR CODE HERE    H, W = image.shape    # Steps:    #     1. Get SVD of the image    u, s, vt = np.linalg.svd(image)    #     2. Only keep the top `num_values` singular values, and compute `compressed_image`    sigular = np.diag(s[:num_values])       # 格式为对角阵    compressed_image = u[:, :num_values].dot(sigular).dot(vt[:num_values, :])    # (H,n) * (n, n) * ( n, W)    #     3. Compute the compressed size, 保留的数据为： (H,n), (sigular), (n, w)    compressed_size = H * num_values + num_values + num_values * W    pass    # END YOUR CODE    assert compressed_image.shape == image.shape, \           &quot;Compressed image and original image don&#39;t have the same shape&quot;    assert compressed_size &gt; 0, &quot;Don&#39;t forget to compute compressed_size&quot;    return compressed_image, compressed_size</code></pre><h1 id="2-k-Nearest-Neighbor"><a href="#2-k-Nearest-Neighbor" class="headerlink" title="2. k-Nearest Neighbor"></a>2. k-Nearest Neighbor</h1><p>使用knn算法分类，使用图片的原始特征，例如像素值；<br>步骤如下：</p><ol><li>计算X_train 和 X_test 的两个元素之间的L2距离</li><li>将数据集划分为5个文件为了交叉验证(cross-validation)</li><li>对于每个文件,对于不同的k，进行预测和评估准确度</li><li>最终获得最佳的k，来评估精确度</li></ol><h2 id="2-1-compute-distance"><a href="#2-1-compute-distance" class="headerlink" title="2.1 compute_distance"></a>2.1 compute_distance</h2><p>函数： <code>def compute_distances(X1, X2)</code><br>参数：</p><ul><li>X1，X2: (M,D)<br>实现：</li><li>L2 距离就是欧式距离</li></ul><pre><code class="python">def compute_distances(X1, X2):    M = X1.shape[0]    N = X2.shape[0]    assert X1.shape[1] == X2.shape[1]    dists = np.zeros((M, N))    # YOUR CODE HERE    # (x-y)^2 == x^2 + y^2 -2xy    x_2 = np.sum(X1**2, axis=1)[:, np.newaxis]     # 增加一个维度    y_2 = np.sum(X2**2, axis=1)    xy_2 = np.dot(X1, X2.T)    dists = x_2 + y_2 - 2 * xy_2    # END YOUR CODE    assert dists.shape == (M, N), &quot;dists should have shape (M, N), got %s&quot; % dists.shape    return dists</code></pre><h2 id="2-2-predict-labels"><a href="#2-2-predict-labels" class="headerlink" title="2.2 predict labels"></a>2.2 predict labels</h2><p>函数： <code>def predict_labels(dists, y_train, k=1)</code><br>参数：</p><ul><li>dists: (num_test, num_train)<br>实现：</li><li>对于每个test寻找最近的train图片，然后统计k个最相似图片的类别，比重占最大的类别就是test的预测类别</li></ul><pre><code class="python">def predict_labels(dists, y_train, k=1):    num_test, num_train = dists.shape    y_pred = np.zeros(num_test, dtype=np.int)    for i in range(num_test):        # A list of length k storing the labels of the k nearest neighbors to        # the ith test point.        closest_y = []        # Use the distance matrix to find the k nearest neighbors of the ith        # testing point, and use self.y_train to find the labels of these        # neighbors. Store these labels in closest_y.        # Hint: Look up the function numpy.argsort.        # Now that you have found the labels of the k nearest neighbors, you        # need to find the most common label in the list closest_y of labels.        # Store this label in y_pred[i]. Break ties by choosing the smaller        # label.        # YOUR CODE HERE        indices = np.argsort(dists[i])      # 最近的label,排序        closest_y = y_train[indices[:k]]    # 选择最近的k个        # y_train 为重复的label，本例子中有800章图片，共16个分类        y_pred[i] = np.bincount(closest_y).argmax()     # 选择k个最相似的label中重复最多的        # END YOUR CODE    return y_pred</code></pre><h2 id="2-3-cross-validation"><a href="#2-3-cross-validation" class="headerlink" title="2.3 cross-validation"></a>2.3 cross-validation</h2><p>选择k个最相似的参数，不能确定k，通过 cross-validation 来选择k；</p><p>将所有训练集分为5个文件，</p><ul><li>80% ： 训练集</li><li>20%%： 验证集</li></ul><p>函数：<code>split_folds(X_train, y_train, num_folds)</code></p><pre><code class="python">def split_folds(X_train, y_train, num_folds):    &quot;&quot;&quot;Split up the training data into `num_folds` folds.    The goal of the functions is to return training sets (features and labels) along with    corresponding validation sets. In each fold, the validation set will represent (1/num_folds)    of the data while the training set represent (num_folds-1)/num_folds.    If num_folds=5, this corresponds to a 80% / 20% split.    For instance, if X_train = [0, 1, 2, 3, 4, 5], and we want three folds, the output will be:        X_trains = [[2, 3, 4, 5],                    [0, 1, 4, 5],                    [0, 1, 2, 3]]        X_vals = [[0, 1],                  [2, 3],                  [4, 5]]    Args:        X_train: numpy array of shape (N, D) containing N examples with D features each        y_train: numpy array of shape (N,) containing the label of each example        num_folds: number of folds to split the data into    jeturns:        X_trains: numpy array of shape (num_folds, train_size * (num_folds-1) / num_folds, D)        y_trains: numpy array of shape (num_folds, train_size * (num_folds-1) / num_folds)        X_vals: numpy array of shape (num_folds, train_size / num_folds, D)        y_vals: numpy array of shape (num_folds, train_size / num_folds)    &quot;&quot;&quot;    assert X_train.shape[0] == y_train.shape[0]    validation_size = X_train.shape[0] // num_folds    training_size = X_train.shape[0] - validation_size    X_trains = np.zeros((num_folds, training_size, X_train.shape[1]))    y_trains = np.zeros((num_folds, training_size), dtype=np.int)    X_vals = np.zeros((num_folds, validation_size, X_train.shape[1]))    y_vals = np.zeros((num_folds, validation_size), dtype=np.int)    # YOUR CODE HERE    # Hint: You can use the numpy array_split function.    # 例如 [1, 2, 3, 4,5]    # [1,2,3,4: 5]    # [2,3,4,5: 1]    # [3,4,5,1: 2]    # [4,5,1,2: 3]    # [5,1,2,3: 4] 这五种组合方式    X_num_folds = np.array(np.array_split(X_train, num_folds))    y_num_folds = np.array(np.array_split(y_train,num_folds))    for i in range(num_folds):        X_trains[i] = X_num_folds[(np.arange(num_folds) != i)].reshape((-1,X_trains.shape[-1]))        X_vals[i] = X_num_folds[i]        y_trains[i] = y_num_folds[(np.arange(num_folds) != i)].reshape((-1, y_trains.shape[-1]))        y_vals[i] = y_num_folds[i]    # END YOUR CODE    return X_trains, y_trains, X_vals, y_vals</code></pre><h1 id="3-PCA"><a href="#3-PCA" class="headerlink" title="3. PCA"></a>3. PCA</h1><p>PCA方法的总结：</p><ol><li>标准化数据</li><li>计算特征向量和特征值 </li><li>按照降序将特征值排序，选择前k个特征值和对应的特征向量，</li><li>建立映射矩阵W,从选择的k个特征向量</li><li>将原始的数据集X通过W降低维度为K的子空间Y</li></ol><h2 id="3-1-Eigen-decomposition"><a href="#3-1-Eigen-decomposition" class="headerlink" title="3,1 Eigen decomposition"></a>3,1 Eigen decomposition</h2><p>类：<code>class PCA(object)</code></p><p>函数：<code>def _eigen_decomp(self, X)</code><br>返回值：</p><ul><li>e_vecs: 特征向量(D,D)</li><li>e_vals: 特征值：(D,)<br>实现： </li><li>主要功能是执行特征协方差矩阵的特征分解</li></ul><pre><code class="python">    def _eigen_decomp(self, X):        N, D = X.shape        e_vecs = None        e_vals = None        # YOUR CODE HERE        # Steps: 计算步骤如下        #     1. compute the covariance matrix of X, of shape (D, D)        matrix = np.matmul(X.T, X) / (X.shape[0] - 1)        # 协方差矩阵 (D,D)        #     2. compute the eigenvalues and eigenvectors of the covariance matrix        e_vals, e_vecs = np.linalg.eig(matrix)      # 计算特征值特征向量        #     3. Sort both of them in decreasing order (ex: 1.0 &gt; 0.5 &gt; 0.0 &gt; -0.2 &gt; -1.2)        indices = np.argsort(-e_vals)       # 从达到小排序，序号        e_vals = np.real(e_vals[indices])   # 返回实数        e_vecs = np.real(e_vecs[:, indices])        # END YOUR CODE        # Check the output shapes        assert e_vals.shape == (D,)        assert e_vecs.shape == (D, D)        return e_vecs, e_vals</code></pre><h2 id="3-2-SVD"><a href="#3-2-SVD" class="headerlink" title="3.2 SVD"></a>3.2 SVD</h2><p>函数：<code>def _svd(self, X)</code><br>返回值：<br>返回值：</p><ul><li>e_vecs: 特征向量(D,D)</li><li>e_vals: 特征值：(D,)<br>实现：</li><li>实现svd分解</li></ul><pre><code class="python">    def _svd(self, X):        vecs = None  # shape (D, D)        N, D = X.shape        vals = None  # shape (K,)        # YOUR CODE HERE        # Here, compute the SVD of X        # Make sure to return vecs as the matrix of vectors where each column is a singular vector        # U: X*X.T , V.T = X.T*X, S:square roots of the eigenvalues of U or V.T        u, s, vt = np.linalg.svd(X)        vals = s        vecs = vt.T         # 转置        # END YOUR CODE        assert vecs.shape == (D, D)        K = min(N, D)        assert vals.shape == (K,)        return vecs, vals</code></pre><h2 id="3-3-Dimensionality-Reduction"><a href="#3-3-Dimensionality-Reduction" class="headerlink" title="3.3 Dimensionality Reduction"></a>3.3 Dimensionality Reduction</h2><p>对原始数据集进行降维处理</p><p>函数： <code>def transform(self, X, n_components)</code></p><pre><code class="python"></code></pre><h2 id="3-4-Reconstruction-error-and-captured-variance"><a href="#3-4-Reconstruction-error-and-captured-variance" class="headerlink" title="3.4  Reconstruction error and captured variance"></a>3.4  Reconstruction error and captured variance</h2><p>将降低维度后的数据重新映射到原空间</p><p>函数：<code>def reconstruct(self, X_proj)</code></p><pre><code class="python">    def reconstruct(self, X_proj):        N, n_components = X_proj.shape        X = None        # YOUR CODE HERE        # Steps:        #     1. project back onto the original space of dimension D        X = np.zeros((N, self.W_pca.shape[1]))      # 原空间        X[:, :n_components] = X_proj        #     2. add the mean that we substracted in `transform`        X = X.dot(np.linalg.inv(self.W_pca)) + self.mean        pass        # END YOUR CODE        return X</code></pre><h1 id="4-Fisherface-Linear-Discriminant-Analysis"><a href="#4-Fisherface-Linear-Discriminant-Analysis" class="headerlink" title="4.  Fisherface: Linear Discriminant Analysis"></a>4.  Fisherface: Linear Discriminant Analysis</h1><p>LDA和PCA的区别：</p><ul><li>LDA需要labels，不是完全无监督，PCA是完全无监督的</li><li>PCA保持 <code>maximum variance</code></li><li>LDA保持 <code>discimination between classes</code></li></ul><h2 id="4-1-Dimensionality-Reduction-via-PCA"><a href="#4-1-Dimensionality-Reduction-via-PCA" class="headerlink" title="4.1 Dimensionality Reduction via PCA"></a>4.1 Dimensionality Reduction via PCA</h2><p>对于本例子，N= 800, D = 4096, 为了应用LDA,需要使得D&lt;N,使用PCA来降低维度；</p><h2 id="4-2-Scatter-matrices"><a href="#4-2-Scatter-matrices" class="headerlink" title="4.2 Scatter matrices"></a>4.2 Scatter matrices</h2><p>为了分类，分别计算类内散度矩阵Sw, 和类间散度矩阵Sb</p><p>函数：<code>def _within_class_scatter(self, X, y)</code><br>实现：</p><ul><li>对于每个类，计算协方差矩阵</li><li>最后结果为协方差矩阵之和</li></ul><pre><code class="python">    def _within_class_scatter(self, X, y):        _, D = X.shape        assert X.shape[0] == y.shape[0]        scatter_within = np.zeros((D, D))        for i in np.unique(y):            # YOUR CODE HERE            # Get the covariance matrix for class i, and add it to scatter_within            X_i = X[y == i]     # 类为i            X_i_centered = X_i - np.mean(X_i, axis=0)            S_i = np.matmul(X_i_centered.T, X_i_centered)       # 协方差矩阵            scatter_within += S_i       # 结果            # END YOUR CODE        return scatter_within</code></pre><p>函数： <code>def _between_class_scatter(self, X, y)</code><br>实现： </p><ul><li>计算两个类之间的协方差矩阵</li></ul><pre><code class="python">    def _between_class_scatter(self, X, y):        _, D = X.shape        assert X.shape[0] == y.shape[0]        scatter_between = np.zeros((D, D))        mu = X.mean(axis=0)        for i in np.unique(y):            # YOUR CODE HERE            X_i = X[y==i]           # 类别i            mu_i = np.mean(X_i, axis=0) # 类别i的均值            scatter_between += (len(y[y==i])) * np.matmul((mu_i - mu).T, (mu_i - mu))            # END YOUR CODE        return scatter_between</code></pre><h2 id="4-3-Solving-generalized-Eigenvalue-problem"><a href="#4-3-Solving-generalized-Eigenvalue-problem" class="headerlink" title="4.3 Solving generalized Eigenvalue problem"></a>4.3 Solving generalized Eigenvalue problem</h2><p>拟合训练数据<br>计算完类内散度矩阵Sw，类间散度矩阵Sb 后，计算矩阵Sw^-1Sb的特征向量和特征矩阵；<br>新的</p><p>函数： <code>def fit(self, X, y)</code></p><pre><code class="python">    def fit(self, X, y):        N, D = X.shape        scatter_between = self._between_class_scatter(X, y)        scatter_within = self._within_class_scatter(X, y)        e_vecs = None        # YOUR CODE HERE        # Solve generalized eigenvalue problem for matrices `scatter_between` and `scatter_within`        # Use `scipy.linalg.eig` instead of numpy&#39;s eigenvalue solver.        # Don&#39;t forget to sort the values and vectors in descending order.        e_vals, e_vecs = scipy.linalg.eig(np.linalg.inv(scatter_within).dot(scatter_between))        # END YOUR CODE        sorting_order = np.argsort(e_vals)[::-1]        e_vals = e_vals[sorting_order]        e_vecs = e_vecs[:,sorting_order]        # 从大到小排序        self.W_lda = e_vecs        # Check that the shape of `self.W_lda` is correct        assert self.W_lda.shape == (D, D)        # Each column of `self.W_lda` should have norm 1 (each one is an eigenvector)        for i in range(D):            assert np.allclose(np.linalg.norm(self.W_lda[:, i]), 1.0)</code></pre><hr>]]></content:encoded>
      
      <comments>https://jingxa.github.io/2018/12/08/CS131-Homework-6/#disqus_thread</comments>
    </item>
    
    <item>
      <title>CS131_Homework_5</title>
      <link>https://jingxa.github.io/2018/12/08/CS131-Homework-5/</link>
      <guid>https://jingxa.github.io/2018/12/08/CS131-Homework-5/</guid>
      <pubDate>Sat, 08 Dec 2018 08:14:42 GMT</pubDate>
      <description>
      
        
        
          &lt;blockquote&gt;
&lt;p&gt;cs131 hw5&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;主要内容&quot;&gt;&lt;a href=&quot;#主要内容&quot; class=&quot;headerlink&quot; title=&quot;主要内容&quot;&gt;&lt;/a&gt;主要内容&lt;/h1&gt;&lt;p&gt;本节主要内容是&lt;code&gt;
        
      
      </description>
      
      <content:encoded><![CDATA[<blockquote><p>cs131 hw5</p></blockquote><hr><h1 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h1><p>本节主要内容是<code>clustering</code>和<code>image segmentation</code></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li><a href="https://github.com/mikucy/CS131/tree/master/hw5_release" target="_blank" rel="noopener">mikucy/CS131</a></li><li><a href="https://github.com/wwdguu/CS131_homework/blob/master/hw5_release/segmentation.py" target="_blank" rel="noopener">wwdguu/CS131_homework</a></li><li><a href="https://github.com/Hugstar/Solutions-Stanford-cs131-Computer-Vision-Foundations-and-Application/blob/master/hw5_release/segmentation.py" target="_blank" rel="noopener">Hugstar/Solutions-Stanford-cs131-Computer-Vision-Foundations-and-Application</a></li></ul><h1 id="1-clustering-algorithms"><a href="#1-clustering-algorithms" class="headerlink" title="1. clustering algorithms"></a>1. clustering algorithms</h1><ul><li>K-Means clustering</li><li>Hierarchical Agglomerative Clustering</li></ul><h2 id="1-1-K-Means-clustering"><a href="#1-1-K-Means-clustering" class="headerlink" title="1.1 K-Means clustering"></a>1.1 K-Means clustering</h2><p>函数：<code>kmeans(features, k, num_iters=100)</code><br>参数：</p><ul><li>features: 特征向量 (N, a feature vector)</li><li>k: 聚簇数量</li><li>num_iters:迭代次数<br>返回值:</li><li>assignments: cluster集合</li></ul><p>实现步骤：</p><ol><li>随机初始化 cluster centers</li><li>将每个点分给最近的中心点</li><li>计算每个cluster新的中心点</li><li>重复以上步骤直到没有改变</li></ol><pre><code class="python">def kmeans(features, k, num_iters=100):    N, D = features.shape    assert N &gt;= k, &#39;Number of clusters cannot be greater than number of points&#39;    # Randomly initalize cluster centers    idxs = np.random.choice(N, size=k, replace=False)    centers = features[idxs]        # 1. 随机中心点    assignments = np.zeros(N)    for n in range(num_iters):        ### YOUR CODE HERE        # 2. 分类        for i in range(N):            dist = np.linalg.norm(features[i] - centers, axis=1)    # 每个点和中心点的距离            assignments[i] = np.argmin(dist)        # 第i个点属于最近的中心点        pre_centers = centers.copy()        # 3. 重新计算中心点        for j in range(k):            centers[j] = np.mean(features[assignments == j], axis=0)        # 4. 验证中心点是否改变        if np.array_equal(pre_centers, centers):            break        ### END YOUR CODE    return assignments </code></pre><p>函数： <code>kmeans_fast(features, k, num_iters=100)</code><br>实现：</p><ul><li>本次实现应该比上个函数快10倍</li><li>使用向量运算,<code>np.repeat</code>和<code>np.argmin</code></li></ul><pre><code class="python">def kmeans_fast(features, k, num_iters=100):    N, D = features.shape    assert N &gt;= k, &#39;Number of clusters cannot be greater than number of points&#39;    # Randomly initalize cluster centers    idxs = np.random.choice(N, size=k, replace=False)    centers = features[idxs]    assignments = np.zeros(N)    for n in range(num_iters):        ### YOUR CODE HERE        # 计算距离        features_tmp = np.tile(features, (k, 1))        # (k*N, ...)        centers_tmp = np.repeat(centers, N, axis=0)     # (N * k, ...)        dist = np.sum((features_tmp - centers_tmp)**2, axis=1).reshape((k, N))      # 每列 即k个中心点        assignments = np.argmin(dist, axis=0)   # 最近        # 计算新的中心点        pre_centers = centers        # 3. 重新计算中心点        for j in range(k):            centers[j] = np.mean(features[assignments == j], axis=0)        # 4. 验证中心点是否改变        if np.array_equal(pre_centers, centers):            break        ### END YOUR CODE    return assignments</code></pre><h2 id="1-2-Hierarchical-Agglomerative-Clustering"><a href="#1-2-Hierarchical-Agglomerative-Clustering" class="headerlink" title="1.2 Hierarchical Agglomerative Clustering"></a>1.2 Hierarchical Agglomerative Clustering</h2><ul><li>此种方法被称为HAC,每个点被初始化为一个cluster center,然后两两合并，直到剩下所需的最后的clusters</li></ul><p>函数： <code>hierarchical_clustering(features, k):</code><br>实现：</p><ol><li>将每个点作为一个cluster</li><li>计算所有cluster的距离，合并两个最近的</li><li>直到最后剩下所期望的clusters</li></ol><p><code>In practice, you probably do not want to use this algorithm to cluster more than 10,000 points.</code></p><p>这次计算中，可以使用scipy的矩阵运算<code>squareform, pdist</code></p><ul><li>squareform: 向量形式距离向量转换为矩形形式距离矩阵,反之亦然。</li><li>pdist:     Pairwise distances between observations in n-dimensional space.</li></ul><pre><code class="python">def hierarchical_clustering(features, k):    N, D = features.shape    assert N &gt;= k, &#39;Number of clusters cannot be greater than number of points&#39;    # Assign each point to its own cluster    assignments = np.arange(N)    centers = np.copy(features)    n_clusters = N    while n_clusters &gt; k:        ### YOUR CODE HERE        dist = pdist(centers)       # 计算相互之间的距离        matrixDist = squareform(dist)   # 将向量形式变化为矩阵形式        matrixDist = np.where(matrixDist != 0.0, matrixDist, 1e10)      # 将0.0的变为1e10,即为了矩阵中相同的点计算的距离去掉        minValue = np.argmin(matrixDist)        # 最小的值的位置        min_i = minValue // n_clusters          # 行号        min_j = minValue - min_i * n_clusters   # 列号        if min_j &lt; min_i:       # 归并到小号的cluster            min_i, min_j = min_j, min_i  # 交换一下        for i in range(N):            if assignments[i] == min_j:                assignments[i] = min_i     # 两者合并        for i in range(N):            if assignments[i] &gt; min_j:                assignments[i] -= 1     # 合并了一个cluster,因此n_clusters减少一位        centers = np.delete(centers, min_j, axis=0)  # 减少一个        centers[min_i] = np.mean(features[assignments == min_i], axis=0)        # 重新计算中心点        n_clusters -= 1     # 减去1        ### END YOUR CODE    return assignments</code></pre><h1 id="2-Pixel-Level-Features"><a href="#2-Pixel-Level-Features" class="headerlink" title="2. Pixel-Level Features"></a>2. Pixel-Level Features</h1><ul><li>对于每个像素，最简单的clustering算法就是计算每个像素的特征，然后比较两个像素的特征值，如果类似，可以归为一个cluster</li></ul><h2 id="2-1-color-features"><a href="#2-1-color-features" class="headerlink" title="2.1 color features"></a>2.1 color features</h2><ul><li>例如，最简单的特征即为color</li></ul><p>函数： <code>color_features(img):</code></p><pre><code>    Args:        img - array of shape (H, W, C)    Returns:        features - array of (H * W, C)</code></pre><p>实现：</p><ul><li>比较两个相邻像素的颜色值即可</li></ul><pre><code class="python">### Pixel-Level Featuresdef color_features(img):    H, W, C = img.shape    img = img_as_float(img)    features = np.zeros((H*W, C))    ### YOUR CODE HERE    features = img.reshape(H * W, C)        # color作为特征    ### END YOUR CODE    return features</code></pre><h2 id="2-2-Color-and-Position-Features"><a href="#2-2-Color-and-Position-Features" class="headerlink" title="2.2 Color and Position Features"></a>2.2 Color and Position Features</h2><ul><li>将color和position联合起来作为特征，即特征为(r,g,b,x,y)</li><li>将颜色值为 [0,1)的值</li><li>对于坐标： 归一化，0 均值和 1 方差</li></ul><p>函数：<code>color_position_features(img)</code></p><p>函数：</p><ul><li>np.mgrid： <a href="https://blog.csdn.net/tymatlab/article/details/79027162" target="_blank" rel="noopener">numpy中mgrid与meshgrid的区别</a></li><li>np.dstack : 堆栈数组按顺序深入（沿第三维）。<a href="https://www.cnblogs.com/nkh222/p/8932369.html" target="_blank" rel="noopener">numpy中的stack操作：hstack()、vstack（）、stack（）、dstack（）、vsplit（）、concatenate（）</a></li><li>np.mean and np.std: 均值和标准差 <a href="https://blog.csdn.net/mvpboss1004/article/details/79249142https://blog.csdn.net/mvpboss1004/article/details/79249142" target="_blank" rel="noopener">numpy中标准差std的神坑</a></li></ul><pre><code class="python">def color_position_features(img):    H, W, C = img.shape    color = img_as_float(img)    features = np.zeros((H*W, C+2))    ### YOUR CODE HERE    # 坐标    cord = np.dstack(np.mgrid[0:H, 0:W]).reshape((H*W, 2))      # mgrid生成坐标，重新格式为（x,y）的二维    features[:, 0:C] = color.reshape((H*W, C))      # r,g,b    features[:, C:C+2] = cord    features = (features - np.mean(features, axis=0)) / np.std(features, axis=0,  ddof = 0)     # 对特征归一化处理    ### END YOUR CODE    return features</code></pre><p>和给出的答案有些不同，是计算方法有错误？</p><hr><h2 id="2-3-Implement-Your-Own-Feature"><a href="#2-3-Implement-Your-Own-Feature" class="headerlink" title="2.3 Implement Your Own Feature"></a>2.3 Implement Your Own Feature</h2><p>略</p><h1 id="3-Quantitative-Evaluation"><a href="#3-Quantitative-Evaluation" class="headerlink" title="3. Quantitative Evaluation"></a>3. Quantitative Evaluation</h1><p>评估算法的实现，本节主要通过测试分割猫和其他背景来评估分割算法的性能。</p><ul><li>将本次问题视为二分问题，将像素划分为foreground和background</li><li>评估：<code>(TP+TN)/(P+N)</code></li></ul><p>函数： <code>def compute_accuracy(mask_gt, mask)</code><br>参数： </p><ul><li>mask_gt: ground truth 分割,每个点为(y,x),维度为(H,W),foreground 为1</li><li>mask: 估计值</li></ul><p>返回值：</p><ul><li>accuracy： 1.0 表示完美分割</li></ul><pre><code class="python">### Quantitative Evaluationdef compute_accuracy(mask_gt, mask):    accuracy = None    ### YOUR CODE HERE    mask_end = mask_gt - mask    count = len(mask_end[np.where(mask_end == 0)])    accuracy = count / (mask_gt.shape[0] * mask_gt.shape[1])    ### END YOUR CODE    return accuracy</code></pre><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本次作业主要完成了聚类和分割的简单算法实现，主要是利用相邻像素的特征之间的“相似性”比较，来划分和聚合。<br>但是，可以从结果中看出，差异明显的图片的分割效果很好，然而，复杂的图片分割效果就很差。</p>]]></content:encoded>
      
      <comments>https://jingxa.github.io/2018/12/08/CS131-Homework-5/#disqus_thread</comments>
    </item>
    
    <item>
      <title>CS131_Homework_4</title>
      <link>https://jingxa.github.io/2018/11/22/CS131-Homework-4/</link>
      <guid>https://jingxa.github.io/2018/11/22/CS131-Homework-4/</guid>
      <pubDate>Thu, 22 Nov 2018 11:33:17 GMT</pubDate>
      <description>
      
        
        
          &lt;blockquote&gt;
&lt;p&gt;cs131 hw4&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;主要内容&quot;&gt;&lt;a href=&quot;#主要内容&quot; class=&quot;headerlink&quot; title=&quot;主要内容&quot;&gt;&lt;/a&gt;主要内容&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;本次作业主要内
        
      
      </description>
      
      <content:encoded><![CDATA[<blockquote><p>cs131 hw4</p></blockquote><hr><h1 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h1><ul><li>本次作业主要内容是 <code>seam carving</code></li></ul><h2 id="1-Image-Reducing-using-Seam-Carving"><a href="#1-Image-Reducing-using-Seam-Carving" class="headerlink" title="1. Image Reducing using Seam Carving"></a>1. Image Reducing using Seam Carving</h2><ul><li>basic idea: 移除不重要的像素点</li><li>定义 <code>unimportant:</code> == <code>less energy pixels</code>， 一般为<code>E = {x轴方向的梯度的绝对值} + {y 轴方向的梯度的绝对值}</code></li></ul><h3 id="1-1-Energy-function"><a href="#1-1-Energy-function" class="headerlink" title="1.1 Energy function"></a>1.1 Energy function</h3><p>函数： <code>energy_function(im)</code></p><p>参数：</p><ul><li>im: 图片 (H, W, 3)</li></ul><p>返回值：</p><ul><li>每个像素点的energy (H, W)</li></ul><p>实现：<br>图像梯度一般也可以用中值差分：</p><pre><code>dx(i,j) = [I(i+1,j) - I(i-1,j)]/2;dy(i,j) = [I(i,j+1) - I(i,j-1)]/2;G(x,y) = dx(i,j) + dy(i,j);</code></pre><ul><li>这里可以使用<code>np.gradient</code></li></ul><pre><code class="python">def energy_function(image):    H, W, _ = image.shape    out = np.zeros((H, W))    gray_image = color.rgb2gray(image)    ### YOUR CODE HERE    dx, dy = np.gradient(gray_image)    out = np.abs(dx) + np.abs(dy)    ### END YOUR CODE    return out</code></pre><h3 id="1-2-compute-cost"><a href="#1-2-compute-cost" class="headerlink" title="1.2 compute cost"></a>1.2 compute cost</h3><p>这一步从图片的顶部到低端计算每个像素的<code>minimal cost</code>,即从顶端到当前像素的最小cost路径</p><p>函数：<code>compute_cost(image, energy, axis=1)</code><br>参数：</p><ul><li>image: </li><li>energy： 数组(H,W)</li><li>axis: 计算维度(width: axis=1 或者 height: axis=0)<br>返回值：</li><li>cost： 每个像素的cost，数组(H,W) </li><li>paths: lowest energy path图， 数组(H,W)，值为-1,0,1 (左上，正上，右上)判断当前像素是否在seam上</li></ul><p>实现：<br>每个像素的cost为<code>M(i,j) = E(i,j) + min(M(i-1,j-1),M(i-1,j),M(i-1,j+1))</code></p><pre><code class="python">def compute_cost(image, energy, axis=1):    energy = energy.copy()    if axis == 0:        energy = np.transpose(energy, (1, 0))    H, W = energy.shape    cost = np.zeros((H, W))    paths = np.zeros((H, W), dtype=np.int)    # Initialization    cost[0] = energy[0]    paths[0] = 0  # we don&#39;t care about the first row of paths    ### YOUR CODE HERE    for i in range(1, H):        M1 = np.r_[[1e10], cost[i-1, 0:W-1]]    # 左边添加一个无穷大        M2 = cost[i-1, :]        M3 = np.r_[cost[i-1, 1:], [1e10]]        # 右边添加一个无穷大        M = np.r_[M1, M2, M3].reshape(3, -1)        cost[i] = energy[i] + np.min(M, axis=0)     # cost        paths[i] = np.argmin(M, axis=0) - 1         # 上一层的最小值为左上角，正上方，右上角    ### END YOUR CODE    if axis == 0:        cost = np.transpose(cost, (1, 0))        paths = np.transpose(paths, (1, 0))    # Check that paths only contains -1, 0 or 1    assert np.all(np.any([paths == 1, paths == 0, paths == -1], axis=0)), \           &quot;paths contains other values than -1, 0 or 1&quot;    return cost, paths</code></pre><h2 id="2-Finding-optimal-seams"><a href="#2-Finding-optimal-seams" class="headerlink" title="2. Finding optimal seams"></a>2. Finding optimal seams</h2><p>通过上一步计算最小能量；需要移除<code>optimal seam</code>， 即最小cost的像素点连成的线</p><h3 id="2-1-Backtrack-seam"><a href="#2-1-Backtrack-seam" class="headerlink" title="2.1 Backtrack seam"></a>2.1 Backtrack seam</h3><p>函数： <code>backtrack_seam(paths, end)</code><br>参数：</p><ul><li>paths:</li><li>end: seam ends(H,end)<br>返回值：</li><li>seam: 数组(H,), </li></ul><p>实现：</p><pre><code>从下到上寻找最小：        - left (value -1)        - middle (value 0)        - right (value 1)</code></pre><p>def backtrack_seam(paths, end):<br>    H, W = paths.shape</p><pre><code># initialize with -1 to make sure that everything gets modifiedseam = - np.ones(H, dtype=np.int)# Initializationseam[H-1] = end     # 最底层的像素点位置(H-1,end)### YOUR CODE HEREfor i in range(H-2,-1,-1):    seam[i] = seam[i+1]+paths[i+1, seam[i+1]]       # 上层点的width坐标### END YOUR CODE# Check that seam only contains values in [0, W-1]assert np.all(np.all([seam &gt;= 0, seam &lt; W], axis=0)), &quot;seam contains values out of bounds&quot;return seam</code></pre><pre><code class="python"></code></pre><h3 id="2-2-Reduce"><a href="#2-2-Reduce" class="headerlink" title="2.2 Reduce"></a>2.2 Reduce</h3><p>移除上一步计算的seam</p><p>函数： <code>remove_seam(image, seam)</code></p><p>参数：</p><ul><li>seam: 每个像素点的位置为<code>image[i, seam[i]]</code></li></ul><p>返回值：</p><ul><li>out: 多维数组，(H,W,C)或者(H,W-1)</li></ul><pre><code class="python">def remove_seam(image, seam):    # Add extra dimension if 2D input    if len(image.shape) == 2:        image = np.expand_dims(image, axis=2)   # 增加一个维度    out = None    H, W, C = image.shape    ### YOUR CODE HERE    out = np.zeros((H, W - 1, C), dtype=image.dtype)        # 返回值，每一行删除一个像素    for i in range(H):        out[i, :seam[i], :]=image[i, :seam[i], :]        out[i, seam[i]:, :]=image[i, seam[i]+1:, :]    ### END YOUR CODE    out = np.squeeze(out)  # remove last dimension if C == 1    # Make sure that `out` has same type as `image`    assert out.dtype == image.dtype, \       &quot;Type changed between image (%s) and out (%s) in remove_seam&quot; % (image.dtype, out.dtype)    return out</code></pre><p>函数：<code>reduce(image, size, axis=1, efunc=energy_function, cfunc=compute_cost)</code></p><p>参数：</p><ul><li>size:根据axis移除像素知道axis的大小值为size</li></ul><p>实现：</p><ul><li>每一次移除一条最小的seam，最终移除size次</li></ul><pre><code class="python">def reduce(image, size, axis=1, efunc=energy_function, cfunc=compute_cost):    out = np.copy(image)    if axis == 0:        out = np.transpose(out, (1, 0, 2))    H = out.shape[0]    W = out.shape[1]    assert W &gt; size, &quot;Size must be smaller than %d&quot; % W    assert size &gt; 0, &quot;Size must be greater than zero&quot;    ### YOUR CODE HERE    while out.shape[1] &gt; size:        energy = efunc(out)         # 首先重新计算energy        cost, paths = cfunc(out, energy)        # 第二步计算cost map        seam = backtrack_seam(paths, np.argmin(cost[-1]))       #计算optimal seam        out = remove_seam(out, seam)                            # 移除    ### END YOUR CODE    assert out.shape[1] == size, &quot;Output doesn&#39;t have the right shape&quot;    if axis == 0:        out = np.transpose(out, (1, 0, 2))    return out</code></pre><h2 id="3-Image-Enlarging"><a href="#3-Image-Enlarging" class="headerlink" title="3. Image Enlarging"></a>3. Image Enlarging</h2><h3 id="3-1-Enlarge-naive"><a href="#3-1-Enlarge-naive" class="headerlink" title="3.1 Enlarge naive"></a>3.1 Enlarge naive</h3><p>扩大图片，可以复制seam</p><p>函数： </p><h4 id="enlarge-naive-image-size-axis-1-efunc-energy-function-cfunc-compute-cost"><a href="#enlarge-naive-image-size-axis-1-efunc-energy-function-cfunc-compute-cost" class="headerlink" title="enlarge_naive(image, size, axis=1, efunc=energy_function, cfunc=compute_cost)"></a><code>enlarge_naive(image, size, axis=1, efunc=energy_function, cfunc=compute_cost)</code></h4><p>增大图片某axis到size大小<br>Use functions:</p><pre><code>- efunc- cfunc- backtrack_seam- duplicate_seam</code></pre><p>返回值： out数组(size, W, c)如果axis=0, 或 (H, size, c) 如果axis=1</p><h4 id="enlarge-image-size-axis-1-efunc-energy-function-cfunc-compute-cost"><a href="#enlarge-image-size-axis-1-efunc-energy-function-cfunc-compute-cost" class="headerlink" title="enlarge(image, size, axis=1, efunc=energy_function, cfunc=compute_cost)"></a><code>enlarge(image, size, axis=1, efunc=energy_function, cfunc=compute_cost)</code></h4><p>寻找多条lowest energy seam<br>Use functions:</p><pre><code>- find_seams- duplicate_seam</code></pre><p>返回值： out数组(size, W, c)如果axis=0, 或 (H, size, c) 如果axis=1    </p><pre><code class="python">def enlarge_naive(image, size, axis=1, efunc=energy_function, cfunc=compute_cost):    out = np.copy(image)    if axis == 0:        out = np.transpose(out, (1, 0, 2))    H = out.shape[0]    W = out.shape[1]    assert size &gt; W, &quot;size must be greather than %d&quot; % W    ### YOUR CODE HERE    while out.shape[1] &lt; size:        energy = efunc(out)         # 首先重新计算energy        cost, paths = cfunc(out, energy)        # 第二步计算cost map        seam = backtrack_seam(paths, np.argmin(cost[-1]))       #计算optimal seam        out = duplicate_seam(out, seam)                            # 复制    ### END YOUR CODE    if axis == 0:        out = np.transpose(out, (1, 0, 2))    return outdef enlarge(image, size, axis=1, efunc=energy_function, cfunc=compute_cost):    out = np.copy(image)    # Transpose for height resizing    if axis == 0:        out = np.transpose(out, (1, 0, 2))    H, W, C = out.shape    assert size &gt; W, &quot;size must be greather than %d&quot; % W    assert size &lt;= 2 * W, &quot;size must be smaller than %d&quot; % (2 * W)    ### YOUR CODE HERE    seams = find_seams(out, size - W)       # 寻找size - W 条 seam    seams = np.expand_dims(seams, axis=2)   # (H,W)    for i in range(size - W):        out = duplicate_seam(out, np.where(seams == i+1)[1])        # 复制当前 seam        seams = duplicate_seam(seams, np.where(seams == i+1)[1])    # seam也复制    ### END YOUR CODE    if axis == 0:        out = np.transpose(out, (1, 0, 2))    return out    </code></pre><h2 id="4-Faster-reduce"><a href="#4-Faster-reduce" class="headerlink" title="4. Faster reduce"></a>4. Faster reduce</h2><ul><li>【暂时没有搞明白 ， 使用别人的代码】</li></ul><p>函数： <code>reduce_fast(image, size, axis=1, efunc=energy_function, cfunc=compute_cost)</code></p><h2 id="5-Forward-Energy"><a href="#5-Forward-Energy" class="headerlink" title="5. Forward Energy"></a>5. Forward Energy</h2><p>函数：<code>compute_forward_cost(image, energy)</code></p><p>实现：</p><pre><code>M1: M(i-1,j-1)M2: M(i-1,j)M3: M(i-1,j+1)CL(i,j): |I(i,j+1) - I(i,j-1)| + |I(i-1,j) - I(i,j-1)|CL(i,j): |I(i,j+1) - I(i,j-1)| + |I(i-1,j) - I(i,j+1)|CV(i,j): |I(i,j+1) - I(i,j-1)|M(i,j) = min{{M1+CL(i,j)}, {M2 + Cv(i,j)}, {M3+CR(i,j)}}</code></pre><hr>]]></content:encoded>
      
      <comments>https://jingxa.github.io/2018/11/22/CS131-Homework-4/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
