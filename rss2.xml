<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Jingxa&#39;s Blog</title>
    <link>https://jingxa.github.io/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>单影无人相依偎！</description>
    <pubDate>Wed, 31 Oct 2018 07:36:50 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>CS131-Homework-3</title>
      <link>https://jingxa.github.io/2018/10/29/CS131-Homework-3/</link>
      <guid>https://jingxa.github.io/2018/10/29/CS131-Homework-3/</guid>
      <pubDate>Mon, 29 Oct 2018 02:12:08 GMT</pubDate>
      <description>
      
        
        
          &lt;blockquote&gt;
&lt;p&gt;cs131 hw3&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;内容&quot;&gt;&lt;a href=&quot;#内容&quot; class=&quot;headerlink&quot; title=&quot;内容&quot;&gt;&lt;/a&gt;内容&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Harries corner 
        
      
      </description>
      
      <content:encoded><![CDATA[<blockquote><p>cs131 hw3</p></blockquote><hr><h1 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h1><ul><li>Harries corner detector</li><li>RANSAC</li><li>HoG descriptor</li></ul><h1 id="相关：-Panaorama-Stitching-全景拼接"><a href="#相关：-Panaorama-Stitching-全景拼接" class="headerlink" title="相关： Panaorama Stitching(全景拼接)"></a>相关： Panaorama Stitching(全景拼接)</h1><p>全景图片的拼接过程一般如下：</p><ol><li>使用<code>Harries detector</code>寻找关键点</li><li>建立每个关键点的descriptor, 比较两幅图片的descriptor，寻找keypoints对</li><li>在keypoints对上，使用 <code>least-squares method</code>求解仿设变换矩阵</li><li>使用RANSAC优化矩阵，然后对多幅图片进行变换，得到全景</li></ol><p>额外的任务：</p><ul><li>实现HoG descriptor</li></ul><h2 id="1-Harris-Corner-Detector"><a href="#1-Harris-Corner-Detector" class="headerlink" title="1. Harris Corner Detector"></a>1. Harris Corner Detector</h2><p>harris 的基本算法分为如下几步：</p><pre><code class="python">def harris_corners(img, window_size=3, k=0.04):    H, W = img.shape    window = np.ones((window_size, window_size))    response = np.zeros((H, W))    #第一步： 偏导数    dx = filters.sobel_v(img)    dy = filters.sobel_h(img)    # 第二步： 偏导数乘积    dxx = dx * dx    dyy = dy * dy    dxy = dx * dy    # 第三步： 形成矩阵    mxx = convolve(dxx, window)    mxy = convolve(dxy, window)    myy = convolve(dyy, window)      #加权计算    # 第四步： 计算response    for i in range(H):        for j in range(W):            M = np.array([[mxx[i, j], mxy[i, j]], [mxy[i, j], myy[i, j]]])            response[i, j] = np.linalg.det(M) - k * np.trace(M) ** 2    return response</code></pre><h2 id="2-Describing-and-Matching-Keypoints"><a href="#2-Describing-and-Matching-Keypoints" class="headerlink" title="2. Describing and Matching Keypoints"></a>2. Describing and Matching Keypoints</h2><h3 id="2-1-Descriptors-创建"><a href="#2-1-Descriptors-创建" class="headerlink" title="2.1 Descriptors 创建"></a>2.1 Descriptors 创建</h3><p>函数： <code>simple_descriptor()</code><br>参数： </p><ul><li>patch: 灰度图片的一个patch，大小（H,W）</li></ul><p>返回值：</p><ul><li>features： 1D的数组(H * W)</li></ul><p>实现：</p><ul><li>将H * W 的patch排列为1D，然后归一化,采用高斯核（x - u）/delta </li></ul><pre><code class="python">def simple_descriptor(patch):    feature = []    patch = patch.reshape(-1)    mean = np.mean(patch)       # 均值    delta = np.std(patch)       # 标准差    if delta &gt; 0.0:        patch = (patch - mean) / delta    else:        patch = patch - mean    feature = list(patch)    return feature</code></pre><h2 id="2-2-descriptors匹配"><a href="#2-2-descriptors匹配" class="headerlink" title="2.2 descriptors匹配"></a>2.2 descriptors匹配</h2><ul><li>通过计算两个descriptors的欧式距离来比较</li><li>点A：最近点B,第二近点C</li><li>如果（dist(A-B)/ dist(A-C)） 小于某个阈值，说明A与B为最佳配对</li></ul><p>函数：<code>match_descriptors(desc1, desc2, threshold=0.5)</code><br>参数：</p><ul><li>desc1,desc2: 两个描述符数组</li><li>thresh： 阈值</li></ul><p>返回值：</p><ul><li>matches:配对点</li></ul><pre><code class="python">def match_descriptors(desc1, desc2, threshold=0.5):    matches = []    N = desc1.shape[0]    dists = cdist(desc1, desc2)     # 每个向量的欧式距离 N * N    idx = np.argsort(dists, axis=1)     # 从小到大对dist排序，返回序号， N * N    for i in range(N):        closed_dist = dists[i, idx[i, 0]]        second_dist = dists[i, idx[i, 1]]        if(closed_dist &lt; threshold * second_dist):      # 比较            matches.append([i, idx[i, 0]])    matches = np.array(matches)    return matches</code></pre><h2 id="3-Transformation-Estimation"><a href="#3-Transformation-Estimation" class="headerlink" title="3. Transformation Estimation"></a>3. Transformation Estimation</h2><p>-通过匹配点来估计两个图片的仿射变换，使用最小二乘法来计算</p><p>函数： <code>fit_affine_matrix(p1, p2)</code></p><p>参数： </p><ul><li>p1, p2: 两组对应点</li></ul><p>返回值：</p><ul><li>H： 仿射变换</li></ul><pre><code class="python">def fit_affine_matrix(p1, p2):    assert (p1.shape[0] == p2.shape[0]),\        &#39;Different number of points in p1 and p2&#39;    p1 = pad(p1)    p2 = pad(p2)        # 齐次矩阵    H = np.linalg.lstsq(p2, p1)[0]    # Sometimes numerical issues cause least-squares to produce the last    # column which is not exactly [0, 0, 1]    H[:,2] = np.array([0, 0, 1])    return H</code></pre><h2 id="4-RANSAC-优化匹配"><a href="#4-RANSAC-优化匹配" class="headerlink" title="4. RANSAC 优化匹配"></a>4. RANSAC 优化匹配</h2><p>一般步骤为：</p><ol><li>选择随机的匹配对</li><li>计算仿射变换矩阵</li><li>寻找RANSAC拟合的inliers</li><li>重复直到找到包含inliers最多的模型</li><li>重新计算最小二乘误差 在内点</li></ol><p>函数： <code>ransac(keypoints1, keypoints2, matches, n_iters=200, threshold=20)</code></p><p>参数：</p><ul><li>keypoints1, keypoints2: 关键点</li><li>matches： 对应点序号</li><li>n_iters: 迭代次数</li><li>thresh:  阈值</li></ul><p>返回值：</p><ul><li>H : 仿射变换矩阵</li></ul><p>实现：</p><ul><li>RANSAC寻找的就是使得配对点数尽可能多的H。</li><li>首先，随机挑选对应对，使用最小二乘求解一个变换h,计算通过当前h，能够得到的最大对应点的数量</li><li>然后，迭代n_iters次，选择最大对应点数量的变换矩阵矩阵H</li><li>另一种实现方式，可以查看[cs231A_Homework_3.2]</li></ul><pre><code class="python">def ransac(keypoints1, keypoints2, matches, n_iters=200, threshold=20):    # Copy matches array, to avoid overwriting it    orig_matches = matches.copy()    matches = matches.copy()    N = matches.shape[0]    print(N)    n_samples = int(N * 0.2)                            # 随机取样    matched1 = pad(keypoints1[matches[:, 0]])            #第一列的序号 齐次矩阵    matched2 = pad(keypoints2[matches[:, 1]])            # 第二列的序号    max_inliers = np.zeros(N)    n_inliers = 0    # RANSAC iteration start    for i in range(n_iters):        temp_max = np.zeros(N, dtype=np.int32)      # 临时变量        temp_n = 0        idx = np.random.choice(N, n_samples, replace=False)     # 随机抽取 n_samples        p1 = matched1[idx, :]        p2 = matched2[idx, :]        H = np.linalg.lstsq(p2, p1)[0]              # 临时变换 H        H[:, 2] = np.array([0, 0, 1])        temp_max = np.linalg.norm(matched2.dot(H) - matched1, axis=1) ** 2 &lt; threshold      # 计算当前对应点的数量        temp_n = np.sum(temp_max)        if temp_n &gt; n_inliers:          # 保存最大数量            max_inliers = temp_max.copy()            n_inliers = temp_n    H = np.linalg.lstsq(matched2[max_inliers], matched1[max_inliers])[0]    H[:, 2] = np.array([0, 0, 1])    return H, matches[max_inliers]</code></pre><h2 id="5-HoG"><a href="#5-HoG" class="headerlink" title="5. HoG"></a>5. HoG</h2><p>HoG的计算步骤一般如下：</p><ol><li>计算图片的x,y方向上的偏导数，使用<code>skimage.filters</code></li><li>将图片划分多个block,每个block划分为cells,计算每个cell的梯度直方图</li><li>flatten 每个block为特征向量</li><li>归一化</li></ol><p>函数：<code>hog_descriptor(patch, pixels_per_cell=(8,8))</code><br>参数：</p><ul><li>patch : 图片(H,W)</li><li>pixels_per_cell: 每个cell的size(M,N)</li></ul><p>返回值：</p><ul><li>block : 一个block的特征向量（（H<em>W</em>n_bins） / (M*N)）<br>实现：</li><li>将patch划分为多个cells,计算每个cells的梯度，然后计算梯度直方图</li><li>最后归一化</li></ul><pre><code class="python">def hog_descriptor(patch, pixels_per_cell=(8,8)):    assert (patch.shape[0] % pixels_per_cell[0] == 0),\                &#39;Heights of patch and cell do not match&#39;    assert (patch.shape[1] % pixels_per_cell[1] == 0),\                &#39;Widths of patch and cell do not match&#39;    n_bins = 9    degrees_per_bin = 180 // n_bins             # 8 个方向，每个方向 20 度    # sobel求解梯度    Gx = filters.sobel_v(patch)    Gy = filters.sobel_h(patch)    # 梯度值和方向    G = np.sqrt(Gx**2 + Gy**2)    theta = (np.arctan2(Gy, Gx) * 180 / np.pi) % 180    # Group entries of G and theta into cells of shape pixels_per_cell, (M, N)    #   G_cells.shape = theta_cells.shape = (H//M, W//N)    #   G_cells[0, 0].shape = theta_cells[0, 0].shape = (M, N)    G_cells = view_as_blocks(G, block_shape=pixels_per_cell)            # 划分为block    theta_cells = view_as_blocks(theta, block_shape=pixels_per_cell)    rows = G_cells.shape[0]    cols = G_cells.shape[1]    # For each cell, keep track of gradient histrogram of size n_bins    cells = np.zeros((rows, cols, n_bins))    # Compute histogram per cell    for i in range(rows):        for j in range(cols):            for m in range(pixels_per_cell[0]):                for n in range(pixels_per_cell[1]):                    idx = int(theta_cells[i, j, m, n] // degrees_per_bin)       # 计算当前像素点 位于n_bins 直方图的那个区间                    if idx == 9:    # 180 度                        idx = 8                    cells[i, j, idx] += G_cells[i, j, m, n]             # 统计    cells = (cells - np.mean(cells)) / np.std(cells)        # 归一化    block = cells.reshape(-1)    return block</code></pre><h2 id="6-图片融合"><a href="#6-图片融合" class="headerlink" title="6. 图片融合"></a>6. 图片融合</h2><p>当两张图片融合的时候，可以看到明显的线条，使用一种叫做<strong>linear blending</strong>的方法平滑消除问题。</p><p>基本步骤如下：</p><ol><li>定义 left 和 right margins</li><li>定义一个 weight matrix 对于图片1：<ul><li>左边的权重 等于1</li><li>从left margin 到 right margin， 权重 递减为 1  to 0</li></ul></li><li>定一个 图片2 的 weight matrix<ul><li>在 right margin 的右边，定义 weight = 1</li><li>从right margin to left margin, 权重递减 为 0 to 1</li></ul></li><li>将 weight matrix 应用对应的图片上</li><li>合并图片</li></ol><pre><code class="python">def linear_blend(img1_warped, img2_warped):    &quot;&quot;&quot;    Linearly blend img1_warped and img2_warped by following the steps:    1. Define left and right margins (already done for you)    2. Define a weight matrices for img1_warped and img2_warped        np.linspace and np.tile functions will be useful    3. Apply the weight matrices to their corresponding images    4. Combine the images    Args:        img1_warped: Refernce image warped into output space        img2_warped: Transformed image warped into output space    Returns:        merged: Merged image in output space    &quot;&quot;&quot;    out_H, out_W = img1_warped.shape # Height and width of output space    img1_mask = (img1_warped != 0)  # Mask == 1 inside the image    img2_mask = (img2_warped != 0) # Mask == 1 inside the image    # Find column of middle row where warped image 1 ends    # This is where to end weight mask for warped image 1    # np.fliplr 左右翻转； np.argmax:最大值的下标    right_margin = out_W - np.argmax(np.fliplr(img1_mask)[out_H//2, :].reshape(1, out_W), 1)[0] # 最大值列    # Find column of middle row where warped image 2 starts    # This is where to start weight mask for warped image 2    left_margin = np.argmax(img2_mask[out_H//2, :].reshape(1, out_W), 1)[0]    left_matrix = np.array(img1_mask,  dtype=np.float64)        # 非常重要。转换为浮点类型    right_matrix = np.array(img2_mask, dtype=np.float64)    # 渐进变换区域    left_matrix[:, left_margin: right_margin] = np.tile(np.linspace(1, 0, right_margin - left_margin), (out_H, 1))    right_matrix[:, left_margin: right_margin] = np.tile(np.linspace(0, 1, right_margin - left_margin), (out_H, 1))    img1 = left_matrix * img1_warped    img2 = right_matrix * img2_warped    merged = img1 + img2    return merged</code></pre><p>上述方法存在一些问题，因为在两幅图片的margin判断的时候，可能没有取到边界的地方，导致结果图片中存在错误。</p><p>可以使用下面的一种方法，先计算</p><pre><code>merged = img1_warped + img2_warped img1_mask = img1_mask * 1.0img2_mask = img2_mask * 1.0merged = img1_warped + img2_warpedoverlap = (img1_mask * 1.0 + img2_mask)### YOUR CODE HERE#找到填充图像2的左边界，也就是列像素不全为0的位置for col in range(img2_warped.shape[1]):    if not np.all(img2_mask[:,col]==False):        breakleft_region = col right_region = img1.shape[1]width_region = right_region - left_region + 1for col in range(left_region, right_region +1):    for row in range(img2_warped.shape[0]):        alpha = 1 - (col - left_region)/width_region        if img1_mask[row, col] and img2_mask[row, col]:            merged[row, col] = alpha * img1_warped[row, col] + (1 - alpha) * img2_warped[row, col]output = merged        ### END YOUR CODE</code></pre><h2 id="7-全景融合"><a href="#7-全景融合" class="headerlink" title="7. 全景融合"></a>7. 全景融合</h2><ul><li>略</li></ul><h1 id="2-结果"><a href="#2-结果" class="headerlink" title="2. 结果"></a>2. 结果</h1><ul><li><a href="https://github.com/jingxa/CS131_release/blob/master/hw3_release/hw3.ipynb" target="_blank" rel="noopener">my_cs131_hw3</a></li></ul><h1 id="3-参考资料"><a href="#3-参考资料" class="headerlink" title="3. 参考资料"></a>3. 参考资料</h1><ul><li><a href="https://github.com/Jack-An/CS131/blob/master/hw3_release/hw3.ipynb" target="_blank" rel="noopener">Jack-An/CS131</a></li><li><a href="https://github.com/nizihabi/Stanford_CS131_HW/blob/master/hw3_release/hw3.ipynb" target="_blank" rel="noopener">nizihabi/Stanford_CS131_HW</a></li><li><a href="https://github.com/mikucy/CS131/blob/master/hw3_release/hw3.ipynb" target="_blank" rel="noopener">mikucy/CS131</a></li><li><a href="https://github.com/wwdguu/CS131_homework/blob/master/hw3_release/hw3.ipynb" target="_blank" rel="noopener">wwdguu/CS131_homework</a></li></ul><hr>]]></content:encoded>
      
      <comments>https://jingxa.github.io/2018/10/29/CS131-Homework-3/#disqus_thread</comments>
    </item>
    
    <item>
      <title>CS231A_Homework_3.3</title>
      <link>https://jingxa.github.io/2018/10/25/CS231A-Homework-3-3/</link>
      <guid>https://jingxa.github.io/2018/10/25/CS231A-Homework-3-3/</guid>
      <pubDate>Thu, 25 Oct 2018 08:08:20 GMT</pubDate>
      <description>
      
        
        
          &lt;blockquote&gt;
&lt;p&gt;cs231A Homework-3:ps3_code-Histogram-of-Oriented-gradients&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;三、-Histogram-of-Oriented-Gradients
        
      
      </description>
      
      <content:encoded><![CDATA[<blockquote><p>cs231A Homework-3:ps3_code-Histogram-of-Oriented-gradients</p></blockquote><hr><h1 id="三、-Histogram-of-Oriented-Gradients-方向梯度直方图"><a href="#三、-Histogram-of-Oriented-Gradients-方向梯度直方图" class="headerlink" title="三、 Histogram of Oriented Gradients(方向梯度直方图)"></a>三、 Histogram of Oriented Gradients(方向梯度直方图)</h1><p>本部分内容主要实现HoG,然后进行一个简单的应用。</p><ol><li>首先，进行梯度计算；</li><li>计算直方图</li><li>计算HoG特征</li></ol><h2 id="3-1-计算梯度-compute-gradient"><a href="#3-1-计算梯度-compute-gradient" class="headerlink" title="3.1 计算梯度: compute_gradient():"></a>3.1 计算梯度: compute_gradient():</h2><p>参数：</p><ul><li>im: 灰度图片 (H, W)</li></ul><p>返回值：</p><ul><li>angles: 梯度角度 （H-2, W-2）</li><li>magnitudes: 梯度 （H-2, W-2）</li></ul><p>梯度的计算如下：</p><ul><li>给定一个像素和周围8个方向的像素，</li></ul><pre><code>P1 P2 P3P4 P5 P6P7 P8 P9arctan(dy/dx) = (P2 - p8)/(p4 - P6)  [可以使用np.arctan2 : 更稳定]，结果为[-180,180)如果想要得到[0, 180],需要简单地增加180到给负的角度梯度： sqrt((P4 - P6)^2 + (p2- P8)^ 2)</code></pre><pre><code class="python">def compute_gradient(im):    H, W = im.shape    angles = np.zeros((H-2, W-2))    magnitudes = np.zeros((H-2, W-2))       # 建立两个返回值矩阵    for i in range(1, H-1):        for j in range(1, W-1):     # 最外圈的点不计算            top = im[i-1, j]            down = im[i+1, j]            left = im[i, j-1]            right = im[i, j+1]      # 上下左右            angle = np.arctan2(top - down, left - right) * (180 / math.pi)      # 转化为度            magn = np.sqrt((top-down)**2 + (left - right)**2)            if(angle &lt; 0):      # 加180                angle += 180            angles[i-1, j-1] = angle            magnitudes[i-1, j-1] = magn    return angles, magnitudes</code></pre><h2 id="3-2-生成直方图：-generate-histogram"><a href="#3-2-生成直方图：-generate-histogram" class="headerlink" title="3.2 生成直方图： generate_histogram()"></a>3.2 生成直方图： generate_histogram()</h2><p>通过给定的角度矩阵和梯度矩阵，生成角度直方图</p><p>参数：</p><ul><li>angles: （M, N）</li><li>magnitudes:(M,N)</li><li>nbins: 直方图区间数</li></ul><p>返回值：</p><ul><li>histogram： 多维数组(nbins,),包含梯度角的分布</li></ul><p>实现：</p><ol><li>每个直方图应该被划分为0到180度，nbins决定区间数</li><li>迭代，将每个对应的梯度角度放到对应的直方图区间中，为了合理分配到两个相近的区间，使用如下公式：</li></ol><pre><code># center_angle 为 区间的中心，比如 20度的区间，第一个和第二个区间的中心为10,30    histogram[bin1] += magnitude * |angle - center_angle2 | / (180 / nbins)            histogram[bin2] += magnitude * |angle - center_angle1 | / (180 / nbins)</code></pre><p>认为第1个区间和最后一个区间相邻；</p><pre><code class="python">def generate_histogram(angles, magnitudes, nbins = 9):    histogram = np.zeros(nbins)    bin_size = 180 / nbins    center_angles = np.zeros_like(histogram)    for i in range(nbins):        center_angles[i] = (0.5 + i) * bin_size     # 计算每个区间中心    M, N = angles.shape    for m in range(M):        for n in range(N):            angle = angles[m, n]            magn = magnitudes[m, n]            abs_diff = np.abs(center_angles - angle)            # 当 angle 趋于0度            if (180 - center_angles[-1] + angle) &lt; abs_diff[-1]:        # 更新下最后区间的大小                abs_diff[-1] = 180 - center_angles[-1] + angle            # angle趋于180度            if(180 + center_angles[0] - angle) &lt; abs_diff[0]:                abs_diff[0] = 180 - angle + center_angles[0]            # 统计直方图            bin1, bin2 = np.argsort(abs_diff)[0:2]      # 取最近的两个            histogram[bin1] += magn * abs_diff[bin2] / (180.0 / nbins)            histogram[bin2] += magn * abs_diff[bin1] / (180.0 / nbins)    return histogram</code></pre><h2 id="3-3-计算HoG特征：-compute-hog-features"><a href="#3-3-计算HoG特征：-compute-hog-features" class="headerlink" title="3.3 计算HoG特征： compute_hog_features()"></a>3.3 计算HoG特征： compute_hog_features()</h2><p>参数：</p><ul><li>im： 图片矩阵</li><li>pixels_in_cell: 每个单元格的大小</li><li>cenlls_in_block:    每个block中的cell数量</li><li>nbins: histogram bins</li></ul><p>返回值：</p><ul><li>features: hog 特征（H_blocks, W_blocks, cells_in_block <em> cells_in_block </em> nbins）</li></ul><p>实现：</p><ol><li>计算每个图片的梯度和角度</li><li>定义一个cell和block</li><li>定义一个滑动窗口，大小为一个block的大小，滑动步长为block的一半，每个block中的cell存储一个直方图中的梯度，<br>每个block 特征被表示为(cells_in_block， cells_in_block, nbins),也可表示为（ cells_in_block <em> cells_in_block </em> nbins ）<br>,确保归一化</li><li>返回这些所有网格的HoG特征</li></ol><pre><code class="python">def compute_hog_features(im, pixels_in_cell, cells_in_block, nbins):    # 第一步： 获得角度和梯度    angles, magnitudes = compute_gradient(im)    # 第二步： 划分block 和cell    cell_size = pixels_in_cell    block_size = pixels_in_cell * cells_in_block    # 第三步：计算滑动窗口    H, W = angles.shape    stride = int(block_size / 2)    H_blocks = int((H - block_size) / stride) + 1    W_blocks = int((W - block_size) / stride) + 1    # 第四步： 计算每个cell的 histogram    hog_fe = np.zeros((H_blocks, W_blocks, cells_in_block * cells_in_block * nbins))    for h in range(H_blocks):        for w in range(W_blocks):            block_angles = angles[h*stride: h * stride + block_size,                           w * stride: w*stride + block_size]      # 一个block的角度            block_magn = magnitudes[h*stride: h*stride+block_size,                            w*stride: w*stride+block_size]         # 梯度            # 将一个block中的每个cell表示为一个方向直方图            block_hog_fe = np.zeros((cells_in_block, cells_in_block, nbins))            for i in range(cells_in_block):                for j in range(cells_in_block):                        cell_angles = block_angles[i*pixels_in_cell : (i+1)*pixels_in_cell,                                        j*pixels_in_cell : (j+1)*pixels_in_cell]                        cell_magns = block_magn[i*pixels_in_cell : (i+1) * pixels_in_cell,                                        j*pixels_in_cell : (j+1) * pixels_in_cell]                        cell_hist = generate_histogram(cell_angles, cell_magns, nbins)                        block_hog_fe[i, j, :] = cell_hist            # 归一化            block_hog_fe = np.reshape(block_hog_fe, -1)            block_hog_fe /= np.linalg.norm(block_hog_fe)            hog_fe[h, w, :] = block_hog_fe    return hog_fe</code></pre><h2 id="3-4-结果"><a href="#3-4-结果" class="headerlink" title="3.4 结果"></a>3.4 结果</h2><p><img src="https://github.com/jingxa/cs231a_my/raw/master/images/ps3/hog.png" alt="hog"></p><hr>]]></content:encoded>
      
      <comments>https://jingxa.github.io/2018/10/25/CS231A-Homework-3-3/#disqus_thread</comments>
    </item>
    
    <item>
      <title>CS231A_Homework_3.2</title>
      <link>https://jingxa.github.io/2018/10/24/CS231A-Homework-3-2/</link>
      <guid>https://jingxa.github.io/2018/10/24/CS231A-Homework-3-2/</guid>
      <pubDate>Wed, 24 Oct 2018 07:36:00 GMT</pubDate>
      <description>
      
        
        
          &lt;blockquote&gt;
&lt;p&gt;cs231A Homework-3:ps3_code-single-object-recognition&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;二、-Single-Object-Recognition-via-SIFT&quot;&gt;&lt;
        
      
      </description>
      
      <content:encoded><![CDATA[<blockquote><p>cs231A Homework-3:ps3_code-single-object-recognition</p></blockquote><hr><h1 id="二、-Single-Object-Recognition-via-SIFT"><a href="#二、-Single-Object-Recognition-via-SIFT" class="headerlink" title="二、 Single Object Recognition via SIFT"></a>二、 Single Object Recognition via SIFT</h1><p>本节主要使用SIFT实现识别和定位一个给定的对象从一堆测试图片中。</p><h2 id="2-1-关键点匹配"><a href="#2-1-关键点匹配" class="headerlink" title="2.1 关键点匹配"></a>2.1 关键点匹配</h2><p>给定一张图片的keypoint的descriptor ， 和另外多张图片的keypoint descriptors,实现一个算法能够检测关键点匹配</p><p>函数： <code>match_keypoints()</code></p><ul><li>计算一个点和另一幅图片所有点的最近两个点的欧式距离，如果（closed_point_distance / second_closed_point_distance）小于某个阈值，那么说明当前点和最近点closed_point为最佳匹配点</li></ul><p>参数：</p><ul><li>descriptors1: 第一幅图片的 descriptors ,size:(M_1, 128),每一行是一个keypoint 的 descriptor</li><li>descriptors2: 第二幅图片的 descriptors (M_2, 128)</li><li>threshold: 阈值</li></ul><p>返回值：</p><ul><li>matches: 匹配的descriptors数组,返回对应的关键点的序号对（N,2）</li></ul><pre><code class="python">def match_keypoints(descriptors1, descriptors2, threshold = 0.7):    matches_list = []    N = descriptors1.shape[0]    for i in range(N):        des1 = descriptors1[i]      # 一个点的描述符        dist = np.sqrt(np.sum((descriptors2 - des1)**2, axis=1))        # 当前点和另一幅图片所有点的欧式距离        index_sort = np.argsort(dist)           # 按照大小排序，返回序号        closed = index_sort[0]        second_closed = index_sort[1]        if(dist[closed] &lt; threshold * dist[second_closed]):            matches_list.append([i, closed])    matches = np.array(matches_list)    return matches</code></pre><h2 id="2-2-优化匹配"><a href="#2-2-优化匹配" class="headerlink" title="2.2 优化匹配"></a>2.2 优化匹配</h2><p>在上述初始匹配中，有一些错误匹配对，需要移除，这里使用RANSAC算法来计算两幅图片的单应性变换，通过变换，变换到另一幅图片的点应该和对应点的距离差值应该在一个很小的阈值内，如果超出阈值，说明不是对应点</p><p>在两幅图的单应性变换中，可以得到公式如下：</p><p><img src="https://github.com/jingxa/cs231a_my/blob/master/images/ps3/h_trans.png" alt="H"></p><p>因此，对应H矩阵，我们可以建立对他的9个未知量建立方程矩阵（2N*9）,其中可以得到x和y的对应等式，还有1的等式</p><p>函数：<code>refine_match()</code><br>参数：</p><ul><li>keypoints1: 第一幅图片的 关键点descriptors数组，每一行为(u,v, scale, theta)，总大小为(M_1,4)</li><li>keypoints2: 第二幅图片的特征数组</li><li>matches: 初始的配对</li><li>threshold</li><li>num_iterations: 迭代次数</li></ul><p>返回：</p><ul><li>inliers: RANSAC 拟合的对应模型的内点序列</li><li>model: RANSAC 拟合两幅图片的变换矩阵H</li></ul><pre><code class="python">def refine_match(keypoints1, keypoints2, matches, reprojection_threshold = 10,        num_iterations = 1000):    best_model = None       # 最佳模型    best_inliers = []       # 内点集合    best_count = 0          # 最佳匹配对数    sample_size = 4         # H 是 9*9矩阵，除去尺度，有八个自由度，需要4对对应点    P = np.zeros((2 * sample_size, 9))      # 建立 2N*9的矩阵，其中每个对应对建立两个等式    N = matches.shape[0]    for i in range(num_iterations):        sample_indexes = random.sample(range(0, N), sample_size)     # 随机抽取4对样本        sample = matches[sample_indexes, :]        for index, elem in enumerate(sample):       # 获得序列和数据            # 取一对对应点            p1_idx = elem[0]            p2_idx = elem[1]            # 转化为齐次坐标系            point1 = keypoints1[p1_idx, 0:2]        # u,v 坐标            point1 = np.append(point1, 1)           # (u, v, 1)            point2 = keypoints2[p2_idx, 0:2]            u = point2[0]            v = point2[1]            # 建立 P 矩阵            P[2 * index, :] = np.reshape(np.array([point1, np.zeros(3), -u * point1]), -1)            P[2 * index + 1, :] = np.reshape(np.array([np.zeros(3), point1, -v * point1]), -1)        # 求解当前的H矩阵        U, s, VT = np.linalg.svd(P)        H = VT[-1, :].reshape(3, 3)        H /= H[2, 2]        # 归一化        inliers = []        # 当前对应的 内点        count = 0        for index, match in enumerate(matches):     # 对每一对对应点进行变换评估            p1 = keypoints1[match[0], 0:2]      # u,v            p1 = np.append(p1, 1)               # (u,v,1)            p2_pred = H.dot(p1)             # p1 变换后的点            p2_pred /= p2_pred[2]           # 归一化            p2_pred = p2_pred[0:2]          # u,v            p2 = keypoints2[match[1], 0:2]            err = np.sqrt( np.sum( (p2 - p2_pred) ** 2 ) )            if err &lt; reprojection_threshold:        # 此对应点为正确                count += 1                inliers.append(index)        # 记录最佳 H        if count &gt; best_count:            best_model = H            best_inliers = inliers            best_count = count    return best_inliers, best_model</code></pre><h2 id="2-3-get-object-region"><a href="#2-3-get-object-region" class="headerlink" title="2.3 get_object_region()"></a>2.3 get_object_region()</h2><p>使用hough变换获得预测对象的边界盒子</p><p>参数：</p><ul><li>keypoints1: 同上</li><li>keypoints2</li><li>matches</li><li>obj_bbox: (xmin,ymin,xmax, ymax)</li><li>thresh: hough voting 阈值</li></ul><p>返回值：</p><ul><li>cx: 盒子中心x坐标数组</li><li>cy: 盒子中心y坐标数组</li><li>w: 盒子宽度数组</li><li>h: 盒子高度数组</li><li>orient: 盒子方向数组</li></ul><p>Hough Voting的思想就是将参数空间划分为子网格，然后统计子网格的落点计数，在此函数中：</p><ol><li>首先计算每一对对应点的边界盒子数据</li><li>然后获得盒子的最小最大的边界值，然后将整个盒子的可能空间划分为子网格</li><li>然后遍历所有的盒子数据，统计每个子网格的计数，</li><li>遍历每个子网格，如果子网格的落点大于某个阈值，那么就认为这个盒子可以做对象的边界盒子。</li></ol><pre><code class="python">def get_object_region(keypoints1, keypoints2, matches, obj_bbox, thresh = 4,        nbins = 4):    cx, cy, w, h, orient = [], [], [], [], []    # 第一步： 计算边界盒子数组    for match in matches:        p1_idx = match[0]        p2_idx = match[1]        p1 = keypoints1[p1_idx]        p2 = keypoints2[p2_idx]        u1, v1, s1, theta1 = p1[0], p1[1], p1[2], p1[3]     # 两个点的成员        u2, v2, s2, theta2 = p2[0], p2[1], p2[2], p2[3]        # 寻找对应点在img2 中的边界盒子，采用旋转平移进行计算        xmin, ymin, xmax, ymax = obj_bbox        xc1 = (xmin + xmax) / 2.0        yc1 = (ymin + ymax) / 2.0       # 中心点的坐标        w1 = (xmax - xmin) * 1.0        h1 = (ymax - ymin) * 1.0        # 使用浮点表示        O2 = theta2 - theta1        xc2 = (s2/s1) * np.cos(O2) * (xc1 - u1) - (s2/s1) * np.sin(O2) * (yc1 - v1) + u2        yc2 = (s2/s1) * np.sin(O2) * (xc1 - u1) + (s2/s1) * np.cos(O2) * (yc1 - v1) + v2        w2 = (s2/s1) * w1        h2 = (s2/s1) * h1       # 缩放        # 保存到数组中        cx.append(xc2)        cy.append(yc2)        w.append(w2)        h.append(h2)        orient.append(O2)       # 这个方向有点不理解    # 第二步： 计算盒子的子网格划分    cx_min, cx_max = min(cx), max(cx)    cy_min, cy_max = min(cy), max(cy)    w_min, w_max = min(w), max(w)    h_min, h_max = min(h), max(h)    orient_min, orient_max = min(orient), max(orient)    cx_bin_size = (cx_max - cx_min) / float(nbins)    cy_bin_size = (cy_max - cy_min) / float(nbins)    w_bin_size = (w_max - w_min) / float(nbins)    h_bin_size = (h_max - h_min) / float(nbins)    orient_bin_size = (orient_max - orient_min) / float(nbins)    # 第三步： 统计每个子网格的计数    bins = defaultdict(list)        # 由于nbins为4，那么就只计算4个参数    N = matches.shape[0]    for n in range(N):        x_center = cx[n]        y_center = cy[n]        w_center = w[n]        orient_center = orient[n]        for i in range(nbins):            for j in range(nbins):                    for k in range(nbins):                            for l in range(nbins):                                if(cx_min + i * cx_bin_size &lt;= x_center                                        and x_center &lt;= cx_min +(i+1) * cx_bin_size):       # x坐标                                            if(cy_min + j * cy_bin_size &lt;= y_center                                                and y_center &lt;= cy_min + (j+1) * cy_bin_size):                                                    if(w_min + k * w_bin_size &lt;= w_center                                                        and w_center &lt;= w_min + (k+1)*w_bin_size):                                                            if(orient_min + l*orient_bin_size &lt;= orient_center                                                                and orient_center &lt;= orient_min + (l+1) * orient_bin_size):                                                                    bins[(i, j, k, l)].append(n)    # 第四步： 统计    cx0, cy0, w0, h0, orient0 = [], [], [], [], []    for bin_idx in bins:        indices = bins[bin_idx]        votes = len(indices)        if(votes &gt;= thresh):            cx0.append(np.sum(np.array(cx)[indices]) / votes)       # 平均数            cy0.append(np.sum(np.array(cy)[indices]) / votes)            w0.append(np.sum(np.array(w)[indices]) / votes)            h0.append(np.sum(np.array(w)[indices]) / votes)            orient0.append(np.sum(np.array(orient)[indices]) / votes)    return cx0, cy0, w0, h0, orient0</code></pre><h2 id="2-4-结果"><a href="#2-4-结果" class="headerlink" title="2.4 结果"></a>2.4 结果</h2><ul><li><a href="https://github.com/jingxa/cs231a_my/tree/master/ps3_code/code_ps3" target="_blank" rel="noopener">cs231a/ps3</a></li></ul><h2 id="2-5-待解决问题"><a href="#2-5-待解决问题" class="headerlink" title="2.5 待解决问题"></a>2.5 待解决问题</h2><ol><li><p>在第二个函数中，关键点的成员包含[u, v, scale, theta],后面两个是怎么计算的？</p></li><li><p>在边界盒子的子网格统计中，使用了4个参数，但是在5个参数中，我使用h参数替换掉orient参数，结果不对，为什么呢？</p></li></ol><hr>]]></content:encoded>
      
      <comments>https://jingxa.github.io/2018/10/24/CS231A-Homework-3-2/#disqus_thread</comments>
    </item>
    
    <item>
      <title>CS231A-Homework-3.1</title>
      <link>https://jingxa.github.io/2018/10/22/CS231A-Homework-3-1/</link>
      <guid>https://jingxa.github.io/2018/10/22/CS231A-Homework-3-1/</guid>
      <pubDate>Mon, 22 Oct 2018 13:05:31 GMT</pubDate>
      <description>
      
        
        
          &lt;blockquote&gt;
&lt;p&gt;cs231A Homework-3:ps3_code-space_carving&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;一、-Space-carving&quot;&gt;&lt;a href=&quot;#一、-Space-carving&quot; class=
        
      
      </description>
      
      <content:encoded><![CDATA[<blockquote><p>cs231A Homework-3:ps3_code-space_carving</p></blockquote><hr><h1 id="一、-Space-carving"><a href="#一、-Space-carving" class="headerlink" title="一、 Space carving"></a>一、 Space carving</h1><p>本节内容实现一个有效的Space carving 框架的部分。<br>实现过程的步骤划分如下：</p><ol><li>生成初始体素网格</li><li>实现一个视图下的carving</li><li>多视图下的carving</li><li>提高准确率</li><li>验证</li></ol><h2 id="1-生成初始体素网格：form-initial-voxels"><a href="#1-生成初始体素网格：form-initial-voxels" class="headerlink" title="1. 生成初始体素网格：form_initial_voxels()"></a>1. 生成初始体素网格：form_initial_voxels()</h2><p>函数参数</p><ul><li>xlim:x轴大小：<code>[xmin, xmax]</code></li><li>ylim: y轴大小：<code>[ymin, ymax]</code></li><li>zlim: z轴大小：<code>[zmin, zmax]</code></li><li>num_voxels: voxels的数量</li></ul><p>返回值：</p><ul><li>voxels: <code>(N,3)</code> 数组，返回N个体素的位置</li><li>voxel_size: 每个voxel的size</li></ul><blockquote><p>计算voxel_size: 即求得整个立方体体积除去数量，但是可能由于不能除尽</p><ol><li>计算每个voxel的体积，然后开三次方根，得到size</li><li>获得网格中心数组，使用<code>np.linspace</code>和<code>np.meshgrid</code><br>注意小数问题</li></ol></blockquote><ul><li><code>np.linspace(min,max,gap)</code>:在<code>[min,max]</code>区间以gap获得一个数组</li><li><a href="https://blog.csdn.net/lllxxq141592654/article/details/81532855?utm_source=blogxgwz0" target="_blank" rel="noopener">np.meshgrid</a>:获得网格矩阵</li></ul><pre><code class="python">def form_initial_voxels(xlim, ylim, zlim, num_voxels):    x_dim = xlim[1] - xlim[0]    y_dim = ylim[1] - ylim[0]    z_dim = zlim[1] - zlim[0]   # 获取三轴的长度    total_vol = x_dim * y_dim * z_dim  # 体积    one_vol = float(total_vol / num_voxels)    voxel_size = np.cbrt(one_vol)                   # 三次方根    x_voxel_num = np.round(x_dim / voxel_size)      # x轴的cube 数量,round 取整    y_voxel_num = np.round(y_dim / voxel_size)    z_voxel_num = np.round(z_dim / voxel_size)    x = np.linspace(xlim[0] + 0.5 * voxel_size,                    xlim[0] + (0.5 + x_voxel_num - 1) * voxel_size, x_voxel_num )     # 只取前 num_voxels个    y = np.linspace(ylim[0] + 0.5 * voxel_size,                    ylim[0] + (0.5 + y_voxel_num - 1) * voxel_size, y_voxel_num)     # 只取前 num_voxels个    z = np.linspace(zlim[0] + 0.5 * voxel_size,                    zlim[0] + (0.5 + z_voxel_num - 1) * voxel_size, z_voxel_num)     # 只取前 num_voxels个    XX, YY, ZZ = np.meshgrid(x, y, z)    voxels = np.r_[(XX.reshape(-1), YY.reshape(-1), ZZ.reshape(-1))].reshape(3, -1).T   # N *3    return voxels, voxel_size</code></pre><p>提示： 相关的函数：<code>np.meshgrid, np.repeat, np.tile</code></p><h2 id="2-裁剪一个视图中不属于物体的体素：carve"><a href="#2-裁剪一个视图中不属于物体的体素：carve" class="headerlink" title="2. 裁剪一个视图中不属于物体的体素：carve()"></a>2. 裁剪一个视图中不属于物体的体素：carve()</h2><p>函数参数：</p><ul><li>voxels: 体素数组</li><li>camera： 相机位置，存储数据： “silhouette” 矩阵， “image”, 和projection matrix “P”</li></ul><p>返回：</p><ul><li>voxels</li></ul><p>首先需要将3D体素投影到2D图片，移除不属于</p><pre><code class="python">def carve(voxels, camera):    N = voxels.shape[0]    homo_voxels = np.c_[voxels, np.ones((N, 1))].T  # 4 * N    voxel_index = np.arange(0, N)    P = camera.P        # 投影矩阵 3 * 4    img_voxels = P.dot(homo_voxels)   # 3 * N  , 投影到图片    img_voxels /= img_voxels[2, :]   # 归一化    img_voxels = img_voxels[0:2, :].T     # 去掉z轴 N*2    sli = camera.silhouette  # 从camera文件中了解相关信息    sli_idx = np.nonzero(sli)    xmin, xmax = np.min(sli_idx[1]), np.max(sli_idx[1])     # 列    ymin, ymax = np.min(sli_idx[0]), np.max(sli_idx[0])     # 行    voxelX = img_voxels[:, 0]    voxelY = img_voxels[:, 1]    x_filter = np.all([voxelX &gt; xmin, voxelX &lt; xmax], axis=0)       # 一个轴上的逻辑与运算    y_filter = np.all([voxelY &gt; ymin, voxelY &lt; ymax], axis=0)    filter = np.all([x_filter, y_filter], axis=0)    img_voxels = img_voxels[filter, :]      # 过滤大于轮廓矩阵的像素点    voxel_index = voxel_index[filter]     # 过滤掉序号    img_voxels = img_voxels.astype(int)     # 由于归一化，可能有小数，转为整数    sli_filter = (sli[img_voxels[:, 1], img_voxels[:, 0]] == 1)     # (x,y)是否在轮廓矩阵中    voxel_index = voxel_index[sli_filter]    return voxels[voxel_index, :]</code></pre><h2 id="3-获得更好的边界：get-voxel-bounds"><a href="#3-获得更好的边界：get-voxel-bounds" class="headerlink" title="3. 获得更好的边界：get_voxel_bounds()"></a>3. 获得更好的边界：get_voxel_bounds()</h2><p>在初始版本中的方法中，获得voxel的边界是根据相机的位置来计算的【暂时没看明白】</p><p>如果需要获得更好的边界，可以先对初始的立方体进行初始剪切一遍，然后获取得到的voxels，取得最大和最小的点，作为边界值</p><pre><code class="python">def get_voxel_bounds(cameras, estimate_better_bounds = False, num_voxels = 4000):    camera_positions = np.vstack([c.T for c in cameras])    print(camera_positions.shape)    print(&quot;0:&quot;, camera_positions[0])    xlim = [camera_positions[:,0].min(), camera_positions[:,0].max()]    ylim = [camera_positions[:,1].min(), camera_positions[:,1].max()]    zlim = [camera_positions[:,2].min(), camera_positions[:,2].max()]    # For the zlim we need to see where each camera is looking.     camera_range = 0.6 * np.sqrt(diff( xlim )**2 + diff( ylim )**2)    for c in cameras:        viewpoint = c.T - camera_range * c.get_camera_direction()        zlim[0] = min( zlim[0], viewpoint[2] )        zlim[1] = max( zlim[1], viewpoint[2] )    # Move the limits in a bit since the object must be inside the circle    xlim = xlim + diff(xlim) / 4 * np.array([1, -1])    ylim = ylim + diff(ylim) / 4 * np.array([1, -1])    if estimate_better_bounds:        voxels, voxel_size  = form_initial_voxels(xlim, ylim, zlim, num_voxels)        for c in cameras:            voxels = carve(voxels, c)        min_point = np.min(voxels, axis=0) - voxel_size        max_point = np.max(voxels, axis=0) + voxel_size        xlim[0], ylim[0], zlim[0] = min_point[0], min_point[1], min_point[2]        xlim[1], ylim[1], zlim[1] = max_point[0], max_point[1], max_point[2]    return xlim, ylim, zlim</code></pre><h2 id="4-结果"><a href="#4-结果" class="headerlink" title="4. 结果"></a>4. 结果</h2><h3 id="4-1-form-initial-voxels形成一个初始立方体"><a href="#4-1-form-initial-voxels形成一个初始立方体" class="headerlink" title="4.1 form_initial_voxels形成一个初始立方体"></a>4.1 form_initial_voxels形成一个初始立方体</h3><p><img src="https://github.com/jingxa/cs231a_my/raw/master/images/ps3/space_c_a.png" alt="iniital_voxels"></p><h3 id="4-2-carving-一个视角的裁剪"><a href="#4-2-carving-一个视角的裁剪" class="headerlink" title="4.2 carving : 一个视角的裁剪"></a>4.2 carving : 一个视角的裁剪</h3><p><img src="https://github.com/jingxa/cs231a_my/raw/master/images/ps3/space_c_b.png" alt="one_carving"></p><h3 id="4-3-没有优化边界的多视角裁剪"><a href="#4-3-没有优化边界的多视角裁剪" class="headerlink" title="4.3 没有优化边界的多视角裁剪"></a>4.3 没有优化边界的多视角裁剪</h3><p><img src="https://github.com/jingxa/cs231a_my/raw/master/images/ps3/space_c_c.png" alt="muliti_carving"></p><h3 id="4-4-优化边界的多视角裁剪"><a href="#4-4-优化边界的多视角裁剪" class="headerlink" title="4.4 优化边界的多视角裁剪"></a>4.4 优化边界的多视角裁剪</h3><p><img src="https://github.com/jingxa/cs231a_my/raw/master/images/ps3/space_c_d.png" alt="best_carving"></p><h2 id="3-参考文章"><a href="#3-参考文章" class="headerlink" title="3. 参考文章"></a>3. 参考文章</h2><ol><li><a href="https://github.com/zyxrrr/cs231a/blob/master/ps3/space_carving/main.py" target="_blank" rel="noopener">zyxrrr/cs231a</a></li><li><a href="https://github.com/mikucy/CS231A/blob/master/ps3_code/space_carving/main.py" target="_blank" rel="noopener">CS231A/ps3_code/space_carving</a></li><li><a href="https://github.com/chizhang529/cs231a/tree/master/Homework/PS3" target="_blank" rel="noopener">cs231a/Homework/PS3/</a></li></ol><hr>]]></content:encoded>
      
      <comments>https://jingxa.github.io/2018/10/22/CS231A-Homework-3-1/#disqus_thread</comments>
    </item>
    
    <item>
      <title>CS231A_说明</title>
      <link>https://jingxa.github.io/2018/10/22/CS231A-%E8%AF%B4%E6%98%8E/</link>
      <guid>https://jingxa.github.io/2018/10/22/CS231A-%E8%AF%B4%E6%98%8E/</guid>
      <pubDate>Mon, 22 Oct 2018 13:01:11 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;说明&quot;&gt;&lt;a href=&quot;#说明&quot; class=&quot;headerlink&quot; title=&quot;说明&quot;&gt;&lt;/a&gt;说明&lt;/h1&gt;&lt;h3 id=&quot;课程相关&quot;&gt;&lt;a href=&quot;#课程相关&quot; class=&quot;headerlink&quot; title=&quot;课程相关&quot;&gt;&lt;/a&gt;课程相关&lt;/h
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><h3 id="课程相关"><a href="#课程相关" class="headerlink" title="课程相关"></a>课程相关</h3><ul><li><a href="http://web.stanford.edu/class/cs231a/index.html" target="_blank" rel="noopener">CS231A: Computer Vision, From 3D Reconstruction to Recognition</a></li></ul><h3 id="本人解答过程"><a href="#本人解答过程" class="headerlink" title="本人解答过程"></a>本人解答过程</h3><ul><li>具体过程查看博客</li></ul><h3 id="参考答案"><a href="#参考答案" class="headerlink" title="参考答案"></a>参考答案</h3><ul><li><a href="https://github.com/chizhang529/cs231a" target="_blank" rel="noopener">chizhang529/cs231a</a></li><li><a href="https://github.com/mikucy/CS231A" target="_blank" rel="noopener">mikucy/CS231A</a></li><li><a href="https://github.com/zyxrrr/cs231a" target="_blank" rel="noopener">zyxrrr/cs231a</a></li></ul><hr>]]></content:encoded>
      
      <comments>https://jingxa.github.io/2018/10/22/CS231A-%E8%AF%B4%E6%98%8E/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
